A primer to ASP.

Source: [Click here](https://towardsdatascience.com/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24)
Article name: ==Towards Data Science/A step-by-step guide to Speech Recognition and Audio Signal Processing in Python==

- Audio signal is analog signal (i.e. continueous). Machine needs discrete signals to work with.
- Conversion from analog to digital consists of the below 2 processes:
	- **Sampling**
		- It is a procedure used to convert a time-varying (changing with time) signal $s(t)$ to a discrete progression of real numbers $x(n)$. Sampling period ($Ts$) is a term that defines the interval between two successive discrete samples. Sampling Frequency ($fs = 1/Ts$) is the inverse of the sampling period. Common sampling frequencies are 8 kHz, 16 kHz, and 44.1 kHz. A 1 Hz sampling rate means one sample per second and therefore high sampling rates mean better signal quality.
	- **Quantization**
		- This is the process of replacing every real number generated by sampling with an approximation to obtain a finite precision (defined within a range of bits). In the majority of scenarios, 16 bits per sample are used for the representation of a single quantized sample. Therefore, raw audio samples generally have a signal range of -215 to 215 although, during analysis, these values are standardized to the range (-1, 1) for simpler validation and model training. A sample resolution is always measured in bits per sample.


## Speaker recognition infrastructure
A general Speech Recognition system is designed to perform the tasks mentioned below:
![](https://miro.medium.com/max/1400/1*7bL1LloYyA_cekQS6_8Wzw.png)

1. The capture of speech (words, sentences, phrases) given by a human. You can think of this as the Data Acquisition part of any general Machine Learning workflow.
2.  Transforming audio frequencies to make it machine-ready. This process is the data pre-processing part where we clean features of the data for the machine to process it.
3.  Application of Natural Language Processing (NLP) on the acquired data to understand the content of speech.
4.  Synthesis of the recognized words to help the machine speak a similar dialect.

--- 

## Step-1: Reading a file for audio signals
```python
from scipy.io.wavfile import read
```
- 2 purposes:
	- **Recording:** A recording is a file we give to the algorithm as its input. The algorithm then works on this input to analyze its contents and build a speech recognition model. This could be a saved file or a live recording, Python allows for both.
	- **Sampling:** _Sampling_ is the technique used to convert these digital signals into a discrete numeric form. Sampling is done at a certain frequency and it generates numeric signals. Choosing the frequency levels depends on the human perception of sound. For instance, choosing a high frequency implies that the human perception of that audio signal is continuous.

```python
# Using IO module to read Audio Files  
from scipy.io import wavfile  

# Output the parameters: Signal Data Type, Sampling Frequency and Duration  
freq_sample, sig_audio = wavfile.read("/content/Welcome.wav")  
print('\nShape of Signal:', sig_audio.shape)  
print('Signal Datatype:', sig_audio.dtype)  
print('Signal duration:', round(sig_audio.shape[0] / float(freq_sample), 2), 'seconds')  

>>> Shape of Signal: (645632,)   
>>> Signal Datatype: int16   
>>> Signal duration: 40.35 seconds  # Normalize the Signal Value and Plot it on a graph  

pow_audio_signal = sig_audio / np.power(2, 15)  
pow_audio_signal = pow_audio_signal[:100]  
time_axis = 1000 * np.arange(0, len(pow_audio_signal), 1) / float(freq_sample)
plt.plot(time_axis, pow_audio_signal, color='blue')
```
![](https://miro.medium.com/max/788/1*y8qr8Ysh0yMNBiugUhsWZw.png)
	x-axis: Time (ms)
	y-axis: Amplitude

This way we have successfully extracted numerical data from an audio (.wav) file.


## Step-2: Transforming audio frequencies
- The representation of the audio signal we did in the first section represents a **time-domain audio signal.** It shows the intensity (loudness or amplitude) of the sound wave with respect to time. 
- Portions with amplitude = 0, represent silence.
- In terms of sound engineering, amplitude = 0 is the sound of static or moving air particles when no other sound is present in the environment.

**Frequency-Domain Representation:** 
- To better understand an audio signal, it is necessary to look at it through a frequency domain. 
- Fourier Transforms (FT) is a mathematical concept that can decompose this signal and bring out the individual frequencies. This is vital for understanding all the frequencies that are combined together to form the sound we hear. 
- FT gives all the frequencies present in the signal and also shows the magnitude of each frequency.
- Fourier Transform is a mathematical concept that can be used in the conversion of a continuous signal from its _original time-domain_ state to a _frequency-domain_ state. 

![](https://miro.medium.com/max/1400/1*TpXzUaECTsf7yf-YQX6kAQ.png)
**NumPy (np.fft.fft):** 
- This `NumPy` function allows us to compute a 1-D discrete Fourier Transform. 
- The function uses Fast Fourier Transform (FFT) algorithm to convert a given sequence to a Discrete Fourier Transform (DFT). 
- In the file we are processing, we have a sequence of amplitudes drawn from an audio file, that were originally sampled from a continuous signal. 
	- We will use this function to covert this time-domain to a discrete frequency-domain signal.

```python
# Working on the same input file  
# Extracting the length and the half-length of the signal to input to FT
sig_length = len(sig_audio)  
# We will now be using the FT to form the frequency domain of the signal
half_length = np.ceil((sig_length + 1) / 2.0).astype(np.int)  

# Normalize the frequency domain and square it
signal_freq = np.fft.fft(sig_audio)  
signal_freq = abs(signal_freq[0:half_length]) / sig_length  
signal_freq **= 2  
transform_len = len(signal_freq)  # The FT'd signal now needs to be adjusted for both even and odd cases  
if sig_length % 2:  
  signal_freq[1:transform_len] *= 2  
else:  
  signal_freq[1:transform_len-1] *= 2  # Extract the signal's strength in decibels (dB)  
exp_signal = 10 * np.log10(signal_freq)  
x_axis = np.arange(0, half_length, 1) * (freq_sample / sig_length) / 1000.0
plt.plot(x_axis, exp_signal, color='green', linewidth=1)
```
![](https://miro.medium.com/max/794/1*uagxwCrxz25gk9Af2PU5Bw.png)
	x-axis: Frequency representation (kHz)
	y-axis: power of signal (dB)

With this, we were able to apply Fourier Transforms to the Audio input file and subsequently see a frequency domain (frequency against signal strength) representation of the audio.

## Step-3: Extracting features from speech
Once the speech is moved from a time-domain signal to a frequency domain signal, the next step is to convert this frequency domain data into a usable feature vector.

**Mel Frequency Cepstral Coefficients (MFCCs)**
- MFCC is a technique designed to extract features from an audio signal.
- It uses the MEL scale to divide the audio signal’s frequency bands and then extracts coefficients from each individual frequency band, thus, creating a separation between frequencies. MFCC uses the _Discrete Cosine Transform (DCT)_ to perform this operation.
- The MEL scale is established on the human perception of sound, i.e., how the human brain process audio signals and differentiates between the varied frequencies. Let us look at the formation of the MEL scale below.

	**Human voice sound perception:** 
	- An adult human, has a fundamental hearing capacity that ranges from 85 Hz to 255 Hz, and this can further be distinguished between genders (85Hz to 180 Hz for Male and 165 Hz to 255 Hz for females). 
	- Above these fundamental frequencies, there also are harmonics that the human ear processes. 
	- Harmonics are multiplications of the fundamental frequency. These are simple multipliers, for instance, a 100 Hz frequency’s second harmonic will be 200 Hz, third would be 300 Hz, and so on.

> - The rough hearing range for humans is 20Hz to 20KHz and this sound perception is also _non-linear_. 
> - We can distinguish low-frequency sounds better in comparison to high-frequency sounds. 
> - For example, we can clearly state the difference between signals of 100Hz and 200Hz but cannot distinguish between 15000 Hz and 15100 Hz. 
> - To generate tones of varied frequencies we can use the program above or use this [tool.](http://www.szynalski.com/tone-generator/)


**MEL Scale**
- It is a pitch scale (scale of audio signals with varying pitch levels) that is judged by humans on the basis of equality in their distances. It is basically a scale that is derived from human perception. 
- For example, if you were exposed to two sound sources distant from each other, the brain will perceive a distance between these sources without actually seeing them.
- This scale is based on how we humans measure audio signal distances with the sense of hearing. _Because our perception is non-linear, the distances on this scale increase with frequency._

**MEL-spaced Filterbank** 
- To compute the power (strength) of every frequency band, the first step is to distinguish the different feature bands available (done by MFCC). Once these segregations are made, we use filter banks to create partitions in the frequencies and separate them. 
- Filter banks can be created using any specified frequency for partitions. The spacing between filters within a filter bank grows exponentially as the frequency grows. 

>Three discrete mathematical models that go into this processing are the **Discrete Cosine Transform (DCT)**, which is used for decorrelation of filter bank coefficients, also termed as whitening of sound, and **Gaussian Mixture Models — Hidden Markov Models (GMMs-HMMs)** that are a standard for **Automatic Speech Recognition (ASR) algorithms**.


DCT is a linear transformation algorithm, and it will therefore rule out a lot of useful signals, given sound is highly non-linear.

```python
# Installing and importing necessary libraries  
pip install python_speech_features  
from python_speech_features import mfcc, logfbank  

sampling_freq, sig_audio = wavfile.read("Welcome.wav")  # We will now be taking the first 15000 samples from the signal for analysis  
_sig_audio = sig_audio[:15000]  # Using MFCC to extract features from the signal  
_mfcc_feat = mfcc(sig_audio, sampling_freq)  

print('\nMFCC Parameters\nWindow Count =', mfcc_feat.shape[0])  
print('Individual Feature Length =', mfcc_feat.shape[1])  
>>> MFCC Parameters Window Count = 93   
>>> Individual Feature Length = 13

mfcc_feat = mfcc_feat.T  
plt.matshow(mfcc_feat)
```
![](https://miro.medium.com/max/1400/1*tqqK8Wt7vYV246kYD1TOYA.png)
	The first horizontal yellow lines below every segment are the fundamental frequency and at their strongest. Above the yellow line are the harmonics that share the same frequency distance between them.

```python
# Generating filter bank features  
_fb_feat = logfbank(sig_audio, sampling_freq)  
print('\nFilter bank\nWindow Count =', fb_feat.shape[0])  
print('Individual Feature Length =', fb_feat.shape[1])  

>>> Filter bank Window Count = 93  
>>> Individual Feature Length = 26

fb_feat = fb_feat.T  
plt.matshow(fb_feat)
```
![](https://miro.medium.com/max/1400/1*NROIoVKEJBK5matAFANC5g.png)
If we see the two distributions, it is evident that the low frequency and high frequency sound distributions are separated in the second image.

> The MFCC, along with application of Filter Banks is a good algorithm to separate the high and low frequency signals. This expedites the analysis process as we can trim sound signals into two or more separate segments and individually analyze them based on their frequencies.

